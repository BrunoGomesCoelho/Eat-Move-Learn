{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "Install the requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$('#menubar').toggle();\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$('#menubar').toggle();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.1.1 is available.\r\n",
      "You should consider upgrading via the '/Users/ozlemyildiz/opt/anaconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment football failed: No module named 'gfootball'\n"
     ]
    }
   ],
   "source": [
    "# this should now run\n",
    "import kaggle_environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n",
    "from kaggle_environments import evaluate, make, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install stable-baseline3: [docs](https://stable-baselines3.readthedocs.io/en/master/guide/install.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.1.1 is available.\r\n",
      "You should consider upgrading via the '/Users/ozlemyildiz/opt/anaconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install stable-baselines3 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- - - \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-69d324dee492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "print(*env.agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom gym environment and process input\n",
    "\n",
    "To be able to use stable-baselines 3, we must wrap Kaggle environment into a custom one accepted by it;\n",
    "\n",
    "Writing the custom environment turns out to be easier said than done, especially when considering there are multiple processing steps of the input we would like to do along the away - Luckily for us there is a **public preprocessing implementation available by @victordelafuente on [Kaggle](https://www.kaggle.com/victordelafuente/dqn-goose-with-stable-baselines3-pytorch#Using-our-custom-environment)**\n",
    "\n",
    "Note that another options would be to use another lib that more directly implements OpenAI interface (although Kaggles interface isn't exactly the same, it might require less modifications); Options would include [keras-rl](https://github.com/keras-rl/keras-rl) or [OpenAI Baselines](https://github.com/openai/baselines); this option is more outdated since stable-baselines3 seems to have become the standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10,
     22,
     59,
     264
    ]
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, adjacent_positions, row_col, translate, min_distance\n",
    "from kaggle_environments import make\n",
    "\n",
    "from enum import Enum, auto\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CellState(Enum):\n",
    "    EMPTY = 0\n",
    "    FOOD = auto()\n",
    "    HEAD = auto()\n",
    "    BODY = auto()\n",
    "    TAIL = auto()\n",
    "    MY_HEAD = auto()\n",
    "    MY_BODY = auto()\n",
    "    MY_TAIL = auto()\n",
    "    ANY_GOOSE = auto()\n",
    "    \n",
    "\n",
    "class ObservationProcessor:\n",
    "    \n",
    "    def __init__(self, rows, columns, hunger_rate, min_food, debug=False, center_head=True):\n",
    "        self.debug = debug\n",
    "        self.rows, self.columns = rows, columns\n",
    "        self.hunger_rate = hunger_rate\n",
    "        self.min_food = min_food\n",
    "        self.previous_action = -1\n",
    "        self.last_action = -1\n",
    "        self.last_min_distance_to_food = self.rows*self.columns #initial max value to mark no food seen so far\n",
    "        self.center_head = center_head\n",
    "\n",
    "    #***** BEGIN: utility functions ******   \n",
    "    def opposite(self, action):\n",
    "        if action == Action.NORTH:\n",
    "            return Action.SOUTH\n",
    "        if action == Action.SOUTH:\n",
    "            return Action.NORTH\n",
    "        if action == Action.EAST:\n",
    "            return Action.WEST\n",
    "        if action == Action.WEST:\n",
    "            return Action.EAST\n",
    "        raise TypeError(str(action) + \" is not a valid Action.\")\n",
    "        \n",
    "    def _adjacent_positions(self, position):\n",
    "        return adjacent_positions(position, self.columns, self.rows)\n",
    "    \n",
    "    def _min_distance_to_food(self, position, food=None):\n",
    "        food = food if food!=None else self.food\n",
    "        return min_distance(position, food, self.columns)\n",
    "    \n",
    "    def _row_col(self, position):\n",
    "        return row_col(position, self.columns)\n",
    "\n",
    "    def _translate(self, position, direction):\n",
    "        return translate(position, direction, self.columns, self.rows)     \n",
    "\n",
    "    def _preprocess_env(self, obs):\n",
    "        observation = Observation(obs)\n",
    "        \n",
    "        self.my_index = observation.index\n",
    "\n",
    "        if len (observation.geese[self.my_index])>0:\n",
    "            self.my_head = observation.geese[self.my_index][0]\n",
    "            self.my_tail = observation.geese[self.my_index][-1]        \n",
    "            self.my_body = [pos for pos in observation.geese[self.my_index][1:-1]]\n",
    "        else:\n",
    "            self.my_head = -1\n",
    "            self.my_tail = -1\n",
    "            self.my_body = []\n",
    "\n",
    "        \n",
    "        self.geese = [g for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n",
    "        self.geese_cells = [pos for g in self.geese for pos in g if len(g) > 0]\n",
    "        \n",
    "        self.occupied = [p for p in self.geese_cells]\n",
    "        self.occupied.extend([p for p in observation.geese[self.my_index]])\n",
    "        \n",
    "        \n",
    "        self.heads = [g[0] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n",
    "        self.bodies = [pos  for i,g in enumerate(observation.geese) for pos in g[1:-1] if i!=self.my_index and len(g) > 2]\n",
    "        self.tails = [g[-1] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 1]\n",
    "        self.food = [f for f in observation.food]\n",
    "        \n",
    "        self.adjacent_to_heads = [pos for head in self.heads for pos in self._adjacent_positions(head)]\n",
    "        self.adjacent_to_bodies = [pos for body in self.bodies for pos in self._adjacent_positions(body)]\n",
    "        self.adjacent_to_tails = [pos for tail in self.tails for pos in self._adjacent_positions(tail)]\n",
    "        self.adjacent_to_geese = self.adjacent_to_heads + self.adjacent_to_bodies\n",
    "        self.danger_zone = self.adjacent_to_geese\n",
    "        \n",
    "        #Cell occupation\n",
    "        self.cell_states = [CellState.EMPTY.value for _ in range(self.rows*self.columns)]\n",
    "        for g in self.geese:\n",
    "            for pos in g:\n",
    "                self.cell_states[pos] = CellState.ANY_GOOSE.value\n",
    "        for pos in self.heads:\n",
    "                self.cell_states[pos] = CellState.ANY_GOOSE.value\n",
    "        for pos in self.my_body:\n",
    "            self.cell_states[pos] = CellState.ANY_GOOSE.value\n",
    "        self.cell_states[self.my_tail] = CellState.ANY_GOOSE.value\n",
    "                \n",
    "        #detect dead-ends\n",
    "        self.dead_ends = []\n",
    "        for pos_i,_ in enumerate(self.cell_states):\n",
    "            if self.cell_states[pos_i] != CellState.EMPTY.value:\n",
    "                continue\n",
    "            adjacent = self._adjacent_positions(pos_i)\n",
    "            adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n",
    "            num_blocked = sum(adjacent_states)\n",
    "            if num_blocked>=(CellState.ANY_GOOSE.value*3):\n",
    "                self.dead_ends.append(pos_i)\n",
    "        \n",
    "        #check for extended dead-ends\n",
    "        new_dead_ends = [pos for pos in self.dead_ends]\n",
    "        while new_dead_ends!=[]:\n",
    "            for pos in new_dead_ends:\n",
    "                self.cell_states[pos]=CellState.ANY_GOOSE.value\n",
    "                self.dead_ends.append(pos)\n",
    "            \n",
    "            new_dead_ends = []\n",
    "            for pos_i,_ in enumerate(self.cell_states):\n",
    "                if self.cell_states[pos_i] != CellState.EMPTY.value:\n",
    "                    continue\n",
    "                adjacent = self._adjacent_positions(pos_i)\n",
    "                adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n",
    "                num_blocked = sum(adjacent_states)\n",
    "                if num_blocked>=(CellState.ANY_GOOSE.value*3):\n",
    "                    new_dead_ends.append(pos_i)    \n",
    "                    \n",
    "                        \n",
    "    def safe_position(self, future_position):\n",
    "        return (future_position not in self.occupied) and (future_position not in self.adjacent_to_heads) and (future_position not in self.dead_ends)\n",
    "    \n",
    "    def valid_position(self, future_position):\n",
    "        return (future_position not in self.occupied) and (future_position not in self.dead_ends)    \n",
    "\n",
    "    def free_position(self, future_position):\n",
    "        return (future_position not in self.occupied)  \n",
    "    \n",
    "    #***** END: utility functions ******\n",
    "    \n",
    "    def process_env_obs(self, obs):\n",
    "        self._preprocess_env(obs)\n",
    "        \n",
    "        EMPTY = .4\n",
    "        HEAD = -1\n",
    "        BODY = MY_BODY = -.8\n",
    "        TAIL = MY_TAIL = -.5\n",
    "        MY_HEAD = 0\n",
    "        FOOD = 1\n",
    "        RISK = -.5\n",
    "        \n",
    "        #Example: {'remainingOverageTime': 12, 'step': 0, 'geese': [[62], [50]], 'food': [7, 71], 'index': 0}\n",
    "        #observation = [[CellState.EMPTY.value for _ in range(self.columns)] for _ in range(self.rows)]\n",
    "        observation = [[EMPTY for _ in range(self.columns)] for _ in range(self.rows)]\n",
    "        \n",
    "        #Other agents\n",
    "        for pos in self.heads:\n",
    "            r, c = self._row_col(pos)\n",
    "            observation[r][c] = HEAD #CellState.HEAD.value\n",
    "        for pos in self.bodies:\n",
    "            r, c = self._row_col(pos)\n",
    "            observation[r][c] = BODY #CellState.BODY.value\n",
    "        for pos in self.tails:\n",
    "            r, c = self._row_col(pos)\n",
    "            observation[r][c] = TAIL #CellState.TAIL.value\n",
    "\n",
    "        #Me\n",
    "        r, c = self._row_col(self.my_head)\n",
    "        observation[r][c] = MY_HEAD #-1 #CellState.MY_HEAD.value\n",
    "        if self.my_head != self.my_tail:\n",
    "            r, c = self._row_col(self.my_tail)\n",
    "            observation[r][c] = MY_TAIL #CellState.MY_TAIL.value\n",
    "        for pos in self.my_body:\n",
    "            r, c = self._row_col(pos)\n",
    "            observation[r][c] = MY_BODY #CellState.MY_BODY.value\n",
    "            \n",
    "        #Food\n",
    "        for pos in self.food:\n",
    "            r, c = self._row_col(pos)\n",
    "            observation[r][c] = FOOD #CellState.FOOD.value\n",
    "        \n",
    "        \n",
    "        if (self.previous_action!=-1):\n",
    "            aux_previous_pos = self._translate(self.my_head, self.opposite(self.previous_action))\n",
    "            r, c = self._row_col(aux_previous_pos)\n",
    "            if observation[r][c]>0:\n",
    "                observation[r][c] = MY_BODY * .5 #Marked to avoid opposite moves\n",
    "        \n",
    "        #Add risk mark\n",
    "        for pos in self.adjacent_to_heads:\n",
    "            r, c = self._row_col(pos)\n",
    "            if observation[r][c] > 0:\n",
    "                    observation[r][c] = RISK\n",
    "\n",
    "        #Add risk mark\n",
    "        for pos in self.dead_ends:\n",
    "            r, c = self._row_col(pos)\n",
    "            if observation[r][c] > 0:\n",
    "                    observation[r][c] = RISK/2\n",
    "        \n",
    "        if self.center_head:\n",
    "            #NOTE: assumes odd number of rows and columns\n",
    "            head_row, head_col = self._row_col(self.my_head)\n",
    "            v_center = (self.columns // 2) # col 5 on 0-10 (11 columns)\n",
    "            v_roll = v_center - head_col\n",
    "            h_center = (self.rows // 2) # row 3 on 0-7 (7 rows)\n",
    "            h_roll = h_center - head_row\n",
    "            observation = np.roll(observation, v_roll, axis=1)\n",
    "            observation = np.roll(observation, h_roll, axis=0)\n",
    "\n",
    "        return np.array([observation])\n",
    "    \n",
    "    def common_sense_rewards(self, action):\n",
    "        if self.my_head==-1:\n",
    "            if self.debug:\n",
    "                print(\"DIED!!\")\n",
    "            return -2\n",
    "        \n",
    "        reward = 0\n",
    "        future_position = self._translate(self.my_head, action)\n",
    "        check_opposite = (self.previous_action!=-1)\n",
    "        \n",
    "        if future_position in self.occupied:\n",
    "            if self.debug:\n",
    "                print(\"Move to occupied\")\n",
    "            reward = -2 #this action meant death        \n",
    "        elif check_opposite and (self.previous_action==self.opposite(action)): #opposite is currently a patch until Action.opposite works...\n",
    "            if self.debug:\n",
    "                print(\"Move to opposite direction, previous\", self.previous_action, \"vs now\",action)\n",
    "            reward = -2 #this action meant death\n",
    "        elif (future_position in self.food) and (future_position not in self.adjacent_to_heads):\n",
    "            if self.debug:\n",
    "                print(\"Safe move to EAT!\")\n",
    "            reward = 2 #eating is good! \n",
    "        elif future_position in self.dead_ends:\n",
    "            if self.debug:\n",
    "                print(\"Move to dead end\")\n",
    "            reward = 0\n",
    "        else:\n",
    "            min_distance_to_food = self._min_distance_to_food(future_position)\n",
    "            \n",
    "            if min_distance_to_food<=self.last_min_distance_to_food:\n",
    "                if self.debug:\n",
    "                    print(\"Move to food\")\n",
    "                #Removed positive rewards here, eating reward will be considered via gamma (future rewards) if agent gets to food\n",
    "                if future_position in self.danger_zone:\n",
    "                    reward = 0 #0.1 \n",
    "                else:\n",
    "                    reward = 0 #0.2 \n",
    "            else:\n",
    "                #ignore might be moving away, but also the nearest food could have been eaten... NO PENALTY HERE!\n",
    "                reward = 0 \n",
    "                \n",
    "            self.last_min_distance_to_food=min_distance_to_food\n",
    "                \n",
    "        self.previous_action = self.last_action\n",
    "        self.last_action = action\n",
    "        return reward\n",
    "    \n",
    "    \n",
    "#Initial template from: https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html\n",
    "class HungryGeeseEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, dummy_env=False, opponent=['greedy', 'greedy','greedy-goose.py'], action_offset=1, debug=False, defaults=[7,11,10,2]):\n",
    "        super(HungryGeeseEnv, self).__init__()\n",
    "        self.num_envs = 1\n",
    "        self.num_previous_observations = 0\n",
    "        self.debug=debug\n",
    "        self.actions = [action for action in Action]\n",
    "        self.action_offset=action_offset\n",
    "        if not dummy_env:\n",
    "            self.env = make(\"hungry_geese\", debug=self.debug)\n",
    "            self.rows = self.env.configuration.rows\n",
    "            self.columns = self.env.configuration.columns\n",
    "            self.hunger_rate = self.env.configuration.hunger_rate\n",
    "            self.min_food = self.env.configuration.min_food\n",
    "            self.trainer = self.env.train([None, *opponent])\n",
    "        else:\n",
    "            self.env = None\n",
    "            self.rows = defaults[0]\n",
    "            self.columns = defaults[1]\n",
    "            self.hunger_rate = defaults[2]\n",
    "            self.min_food = defaults[3]\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects        \n",
    "        self.action_space = spaces.Discrete(len(self.actions))\n",
    "        self.observation_space = spaces.Box(low=-1, high=1,\n",
    "                                            shape=(self.num_previous_observations+1, self.rows, self.columns), dtype=np.int8)\n",
    "        self.reward_range = (-4, 1)\n",
    "        self.step_num=1        \n",
    "        self.observation_preprocessor = ObservationProcessor(self.rows, self.columns, self.hunger_rate, self.min_food, debug=self.debug, center_head=True)\n",
    "        self.observation = []\n",
    "        self.previous_observation = []\n",
    "    \n",
    "    \n",
    "    def step(self, action):        \n",
    "        action += self.action_offset\n",
    "        action = Action(action)        \n",
    "        cs_rewards = self.observation_preprocessor.common_sense_rewards(action)\n",
    "        if self.debug:\n",
    "            if cs_rewards!=0:\n",
    "                print(\"CS reward\", action.name, self.observation, cs_rewards)\n",
    "            else:\n",
    "                print(\"CS ok\", action.name)\n",
    "        \n",
    "\n",
    "        obs, reward, done, _ = self.trainer.step(action.name)\n",
    "\n",
    "        if len(self.observation)>0:\n",
    "            #Not initial step, t=0\n",
    "            self.previous_observation.append(self.observation)\n",
    "            #Keep list constrained to max length\n",
    "            if len(self.previous_observation)>self.num_previous_observations:\n",
    "                del self.previous_observation[0]\n",
    "            \n",
    "        self.observation = self.observation_preprocessor.process_env_obs(obs)\n",
    "        \n",
    "        if len(self.previous_observation)==0:\n",
    "            #Initial step, t=0\n",
    "            self.previous_observation = [self.observation for _ in range(self.num_previous_observations)]\n",
    "        \n",
    "        info = {}\n",
    "        #if self.debug:\n",
    "        #    print(action, reward, cs_rewards, done, \"\\n\"+\"\\n\".join([str(o) for o in self.observation]))\n",
    "        \n",
    "        env_reward = reward\n",
    "        if len(self.previous_observation)>0:\n",
    "            unique_before, counts_before = np.unique(self.previous_observation[-1], return_counts=True)\n",
    "            unique_now, counts_now = np.unique(self.observation, return_counts=True)\n",
    "            before = dict(zip(unique_before, counts_before))\n",
    "            now = dict(zip(unique_now, counts_now))\n",
    "            count_length = lambda d: d.get(CellState.MY_HEAD.value, 0) + d.get(CellState.MY_BODY.value, 0) + d.get(CellState.MY_TAIL.value, 0)\n",
    "            if count_length(now)>count_length(before):\n",
    "                reward = 2 #Ate\n",
    "            else:\n",
    "                reward = 0 #Just moving\n",
    "            if self.debug:\n",
    "                print(f'{self.step_num} {count_length(now)} {count_length(before)} R {reward}')\n",
    "        else:\n",
    "            reward = 0 # no way to check previuos length use common sense reward on move to food instead ;-)\n",
    "        if done:\n",
    "            #game ended            \n",
    "            if self.observation_preprocessor.my_head == -1:\n",
    "                #DIED, but what final ranking?\n",
    "                rank = len(self.observation_preprocessor.geese)+1\n",
    "                if self.debug:\n",
    "                    print(\"Rank on end\", rank, \"geese\", self.observation_preprocessor.geese)\n",
    "                if rank == 4:\n",
    "                    reward = -2\n",
    "                elif rank == 3:\n",
    "                    reward = 0\n",
    "                elif rank == 2:\n",
    "                    reward = 0\n",
    "                else:\n",
    "                    reward = 100\n",
    "            else:\n",
    "                reward = 1 #survived the game!?\n",
    "        elif reward<1:\n",
    "            reward=1.1 #0 #set to 0 if staying alive is not enough\n",
    "        elif reward>1:\n",
    "            #ate something!!! :-)\n",
    "            reward = 1\n",
    "        \n",
    "        if self.debug and done:\n",
    "            print(\"DONE!\", self.observation, env_reward, reward, cs_rewards)\n",
    "        \n",
    "        reward = cs_rewards if cs_rewards<0 else cs_rewards+reward #if cs_reward<0 use it only      \n",
    "        self.step_num += 1\n",
    "        \n",
    "        if self.num_previous_observations>0:\n",
    "            observations = np.concatenate((*self.previous_observation, self.observation), axis=0)\n",
    "            return observations, reward, done, info\n",
    "        else:\n",
    "            return self.observation, reward, done, info\n",
    "\n",
    "        \n",
    "    def reset(self):\n",
    "        self.observation_preprocessor = ObservationProcessor(self.rows, self.columns, self.hunger_rate, self.min_food, debug=self.debug, center_head=True)\n",
    "        obs = self.trainer.reset()\n",
    "        self.observation = self.observation_preprocessor.process_env_obs(obs)\n",
    "        self.previous_observation = [self.observation for _ in range(self.num_previous_observations)]\n",
    "        return self.observation\n",
    "    \n",
    "    \n",
    "    def render(self,**kwargs):\n",
    "        self.env.render(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HungryGeeseEnv(opponent=['greedy', 'greedy', 'greedy'], debug=False)\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"dqnv1\"\n",
    "m_env = Monitor(env, model_name, allow_early_resets=True) \n",
    "\n",
    "policy_kwargs = dict(\n",
    "    #net_arch = [2000, 1000, 500, 1000, 500, 100],\n",
    "    net_arch = [100, 100, 300, 100, 100, 100, 100, 100],\n",
    "    activation_fn=th.nn.ReLU\n",
    ")\n",
    "\n",
    "TRAIN_STEPS = 1e6\n",
    "alpha_0 = 1e-6\n",
    "alpha_end = 1e-9\n",
    "\n",
    "def learning_rate_f(process_remaining):\n",
    "    #default =  1e-4\n",
    "    initial = alpha_0\n",
    "    final = alpha_end\n",
    "    interval = initial-final\n",
    "    return final+interval*process_remaining\n",
    "\n",
    "params ={\n",
    "    'gamma': .9,\n",
    "    'batch_size': 100,\n",
    "     #'train_freq': 500,\n",
    "    'target_update_interval': 10000,\n",
    "    'learning_rate': learning_rate_f,\n",
    "    'learning_starts': 1000,\n",
    "    'exploration_fraction': .2,\n",
    "    'exploration_initial_eps': .05,\n",
    "    'tau': 1,\n",
    "    'exploration_final_eps': .01,\n",
    "    'buffer_size': 100000,\n",
    "    'verbose': 2,\n",
    "}\n",
    "\n",
    "#coment **params for default parameters\n",
    "trainer = DQN('MlpPolicy', m_env, policy_kwargs=policy_kwargs, **params)\n",
    "\n",
    "#You can check policy architecture with:\n",
    "#print(trainer.policy.net_arch) #prints: [64, 64] for default DQN policy\n",
    "#Or check model.policy\n",
    "print(trainer.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.learn(total_timesteps=100000, callback=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv(f'{model_name}.monitor.csv', header=1, index_col='t')\n",
    "\n",
    "df.rename(columns = {'r':'Episode Reward', 'l':'Episode Length'}, inplace = True) \n",
    "plt.figure(figsize=(20,5))\n",
    "sns.regplot(data=df, y='Episode Reward', x=np.arange(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = trainer.policy.to('cpu').state_dict()\n",
    "print(\"\\n\".join(state_dict.keys())) #use this to check keys ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapted_state_dict ={\n",
    "    new_key : state_dict[old_key]\n",
    "    for old_key in state_dict.keys()\n",
    "    for new_key in [\"layer\"+\".\".join(old_key.split(\".\")[-2:])] #use last 3 components of name\n",
    "    if old_key.find(\"q_net_target.\") != -1 #we only want the policy weights\n",
    "}\n",
    "print(adapted_state_dict.keys())\n",
    "th.save(adapted_state_dict, f'{model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapted_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     17,
     29
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile agent_dqn_1_1.py\n",
    "from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, adjacent_positions, row_col, translate, min_distance\n",
    "from kaggle_environments import make\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from enum import Enum, auto\n",
    "import numpy as np\n",
    "import os\n",
    "import random as rand\n",
    "\n",
    "\n",
    "class CellState(Enum):\n",
    "    EMPTY = 0\n",
    "    FOOD = auto()\n",
    "    HEAD = auto()\n",
    "    BODY = auto()\n",
    "    TAIL = auto()\n",
    "    MY_HEAD = auto()\n",
    "    MY_BODY = auto()\n",
    "    MY_TAIL = auto()\n",
    "    ANY_GOOSE = auto()\n",
    "    \n",
    "\n",
    "class ObservationProcessor:\n",
    "    \n",
    "    def __init__(self, rows, columns, hunger_rate, min_food, debug=False, center_head=True):\n",
    "        self.debug = debug\n",
    "        self.rows, self.columns = rows, columns\n",
    "        self.hunger_rate = hunger_rate\n",
    "        self.min_food = min_food\n",
    "        self.previous_action = -1\n",
    "        self.last_action = -1\n",
    "        self.last_min_distance_to_food = self.rows*self.columns #initial max value to mark no food seen so far\n",
    "        self.center_head = center_head\n",
    "\n",
    "    #***** BEGIN: utility functions ******   \n",
    "    def opposite(self, action):\n",
    "        if action == Action.NORTH:\n",
    "            return Action.SOUTH\n",
    "        if action == Action.SOUTH:\n",
    "            return Action.NORTH\n",
    "        if action == Action.EAST:\n",
    "            return Action.WEST\n",
    "        if action == Action.WEST:\n",
    "            return Action.EAST\n",
    "        raise TypeError(str(action) + \" is not a valid Action.\")\n",
    "        \n",
    "    def _adjacent_positions(self, position):\n",
    "        return adjacent_positions(position, self.columns, self.rows)\n",
    "\n",
    "    \n",
    "    def _min_distance_to_food(self, position, food=None):\n",
    "        food = food if food!=None else self.food\n",
    "        return min_distance(position, food, self.columns)\n",
    "    \n",
    "    \n",
    "    def _row_col(self, position):\n",
    "        return row_col(position, self.columns)\n",
    "\n",
    "    \n",
    "    def _translate(self, position, direction):\n",
    "        return translate(position, direction, self.columns, self.rows)     \n",
    "\n",
    "    \n",
    "    def _preprocess_env(self, obs):\n",
    "        observation = Observation(obs)\n",
    "        \n",
    "        self.my_index = observation.index\n",
    "\n",
    "        if len (observation.geese[self.my_index])>0:\n",
    "            self.my_head = observation.geese[self.my_index][0]\n",
    "            self.my_tail = observation.geese[self.my_index][-1]        \n",
    "            self.my_body = [pos for pos in observation.geese[self.my_index][1:-1]]\n",
    "        else:\n",
    "            self.my_head = -1\n",
    "            self.my_tail = -1\n",
    "            self.my_body = []\n",
    "\n",
    "        \n",
    "        self.geese = [g for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n",
    "        self.geese_cells = [pos for g in self.geese for pos in g if len(g) > 0]\n",
    "        \n",
    "        self.occupied = [p for p in self.geese_cells]\n",
    "        self.occupied.extend([p for p in observation.geese[self.my_index]])\n",
    "        \n",
    "        \n",
    "        self.heads = [g[0] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n",
    "        self.bodies = [pos  for i,g in enumerate(observation.geese) for pos in g[1:-1] if i!=self.my_index and len(g) > 2]\n",
    "        self.tails = [g[-1] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 1]\n",
    "        self.food = [f for f in observation.food]\n",
    "        \n",
    "        self.adjacent_to_heads = [pos for head in self.heads for pos in self._adjacent_positions(head)]\n",
    "        self.adjacent_to_bodies = [pos for body in self.bodies for pos in self._adjacent_positions(body)]\n",
    "        self.adjacent_to_tails = [pos for tail in self.tails for pos in self._adjacent_positions(tail)]\n",
    "        self.adjacent_to_geese = self.adjacent_to_heads + self.adjacent_to_bodies\n",
    "        self.danger_zone = self.adjacent_to_geese\n",
    "        \n",
    "        #Cell occupation\n",
    "        self.cell_states = [CellState.EMPTY.value for _ in range(self.rows*self.columns)]\n",
    "        for g in self.geese:\n",
    "            for pos in g:\n",
    "                self.cell_states[pos] = CellState.ANY_GOOSE.value\n",
    "        for pos in self.heads:\n",
    "                self.cell_states[pos] = CellState.ANY_GOOSE.value\n",
    "        for pos in self.my_body:\n",
    "            self.cell_states[pos] = CellState.ANY_GOOSE.value\n",
    "        self.cell_states[self.my_tail] = CellState.ANY_GOOSE.value\n",
    "                \n",
    "        #detect dead-ends\n",
    "        self.dead_ends = []\n",
    "        for pos_i,_ in enumerate(self.cell_states):\n",
    "            if self.cell_states[pos_i] != CellState.EMPTY.value:\n",
    "                continue\n",
    "            adjacent = self._adjacent_positions(pos_i)\n",
    "            adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n",
    "            num_blocked = sum(adjacent_states)\n",
    "            if num_blocked>=(CellState.ANY_GOOSE.value*3):\n",
    "                self.dead_ends.append(pos_i)\n",
    "        \n",
    "        #check for extended dead-ends\n",
    "        new_dead_ends = [pos for pos in self.dead_ends]\n",
    "        while new_dead_ends!=[]:\n",
    "            for pos in new_dead_ends:\n",
    "                self.cell_states[pos]=CellState.ANY_GOOSE.value\n",
    "                self.dead_ends.append(pos)\n",
    "            \n",
    "            new_dead_ends = []\n",
    "            for pos_i,_ in enumerate(self.cell_states):\n",
    "                if self.cell_states[pos_i] != CellState.EMPTY.value:\n",
    "                    continue\n",
    "                adjacent = self._adjacent_positions(pos_i)\n",
    "                adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n",
    "                num_blocked = sum(adjacent_states)\n",
    "                if num_blocked>=(CellState.ANY_GOOSE.value*3):\n",
    "                    new_dead_ends.append(pos_i)    \n",
    "                    \n",
    "                        \n",
    "    def safe_position(self, future_position):\n",
    "        return (future_position not in self.occupied) and (future_position not in self.adjacent_to_heads) and (future_position not in self.dead_ends)\n",
    "    \n",
    "    \n",
    "    def valid_position(self, future_position):\n",
    "        return (future_position not in self.occupied) and (future_position not in self.dead_ends)    \n",
    "\n",
    "    \n",
    "    def free_position(self, future_position):\n",
    "        return (future_position not in self.occupied)  \n",
    "    \n",
    "    #***** END: utility functions ******\n",
    "    \n",
    "    \n",
    "    def process_env_obs(self, obs):\n",
    "        self._preprocess_env(obs)\n",
    "        \n",
    "        EMPTY = .4\n",
    "        HEAD = -1\n",
    "        BODY = MY_BODY = -.8\n",
    "        TAIL = MY_TAIL = -.5\n",
    "        MY_HEAD = 0\n",
    "        FOOD = 1\n",
    "        RISK = -.5\n",
    "        \n",
    "        #Example: {'remainingOverageTime': 12, 'step': 0, 'geese': [[62], [50]], 'food': [7, 71], 'index': 0}\n",
    "        #observation = [[CellState.EMPTY.value for _ in range(self.columns)] for _ in range(self.rows)]\n",
    "        observation = [[EMPTY for _ in range(self.columns)] for _ in range(self.rows)]\n",
    "        \n",
    "        #Other agents\n",
    "        for pos in self.heads:\n",
    "            r, c = self._row_col(pos)\n",
    "            observation[r][c] = HEAD #CellState.HEAD.value\n",
    "        for pos in self.bodies:\n",
    "            r, c = self._row_col(pos)\n",
    "            observation[r][c] = BODY #CellState.BODY.value\n",
    "        for pos in self.tails:\n",
    "            r, c = self._row_col(pos)\n",
    "            observation[r][c] = TAIL #CellState.TAIL.value\n",
    "\n",
    "        #Me\n",
    "        r, c = self._row_col(self.my_head)\n",
    "        observation[r][c] = MY_HEAD #-1 #CellState.MY_HEAD.value\n",
    "        if self.my_head != self.my_tail:\n",
    "            r, c = self._row_col(self.my_tail)\n",
    "            observation[r][c] = MY_TAIL #CellState.MY_TAIL.value\n",
    "        for pos in self.my_body:\n",
    "            r, c = self._row_col(pos)\n",
    "            observation[r][c] = MY_BODY #CellState.MY_BODY.value\n",
    "            \n",
    "        #Food\n",
    "        for pos in self.food:\n",
    "            r, c = self._row_col(pos)\n",
    "            observation[r][c] = FOOD #CellState.FOOD.value\n",
    "        \n",
    "        \n",
    "        if (self.previous_action!=-1):\n",
    "            aux_previous_pos = self._translate(self.my_head, self.opposite(self.previous_action))\n",
    "            r, c = self._row_col(aux_previous_pos)\n",
    "            if observation[r][c]>0:\n",
    "                observation[r][c] = MY_BODY * .5 #Marked to avoid opposite moves\n",
    "        \n",
    "        #Add risk mark\n",
    "        for pos in self.adjacent_to_heads:\n",
    "            r, c = self._row_col(pos)\n",
    "            if observation[r][c] > 0:\n",
    "                    observation[r][c] = RISK\n",
    "\n",
    "        #Add risk mark\n",
    "        for pos in self.dead_ends:\n",
    "            r, c = self._row_col(pos)\n",
    "            if observation[r][c] > 0:\n",
    "                    observation[r][c] = RISK/2        \n",
    "        \n",
    "        if self.center_head:\n",
    "            #NOTE: assumes odd number of rows and columns\n",
    "            head_row, head_col = self._row_col(self.my_head)\n",
    "            v_center = (self.columns // 2) # col 5 on 0-10 (11 columns)\n",
    "            v_roll = v_center - head_col\n",
    "            h_center = (self.rows // 2) # row 3 on 0-7 (7 rows)\n",
    "            h_roll = h_center - head_row\n",
    "            observation = np.roll(observation, v_roll, axis=1)\n",
    "            observation = np.roll(observation, h_roll, axis=0)\n",
    "\n",
    "        return np.array([observation])\n",
    "    \n",
    "    \n",
    "class MyNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNN, self).__init__()\n",
    "        \"\"\"use names generated on adapted saved_dict\n",
    "        dict_keys(['layer0.weight', 'layer0.bias', 'layer2.weight', 'layer2.bias', ...])\n",
    "\n",
    "        net_arch as seen before:\n",
    "          (q_net): QNetwork(\n",
    "            (features_extractor): FlattenExtractor(\n",
    "              (flatten): Flatten(start_dim=1, end_dim=-1)\n",
    "            )\n",
    "            (q_net): Sequential(\n",
    "              (0): Linear(...)\n",
    "              (1): ReLU()\n",
    "              ...\n",
    "            )\n",
    "          )\n",
    "        \"\"\"\n",
    "        self.even_layers = []\n",
    "        net_arch = [77] + [100, 100, 300, 100, 100, 100, 100, 100] + [4]\n",
    "        for inp, out in zip(net_arch[:-1], net_arch[1:]):\n",
    "            self.even_layers.append(nn.Linear(inp, out))\n",
    "        \"\"\"\n",
    "        #net_arch = [2000, 1000, 500, 1000, 500, 100]\n",
    "        self.layer0 = nn.Linear(77, 2000)\n",
    "        self.layer2 = nn.Linear(2000, 1000)\n",
    "        self.layer4 = nn.Linear(1000, 500)\n",
    "        self.layer6 = nn.Linear(500, 1000)\n",
    "        self.layer8 = nn.Linear(1000, 500)\n",
    "        self.layer10 = nn.Linear(500, 100)\n",
    "        self.layer12 = nn.Linear(100, 4)\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.Flatten()(x)  # no feature extractor means flatten (check policy arch on DQN creation)\n",
    "        for layer in self.even_layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.even_layers[-1](x)\n",
    "        return x\n",
    "\n",
    "        \"\"\"\n",
    "        for layer in [self.layer0, self.layer2, self.layer4, self.layer6, self.layer8, self.layer10]:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.layer12(x)\n",
    "        \"\"\"\n",
    "        return x\n",
    "            \n",
    "        \n",
    "def my_dqn(observation, configuration):\n",
    "    global model, obs_prep, last_action, last_observation, previous_observation\n",
    "\n",
    "    #tgz_agent_path = '/kaggle_simulations/agent/'\n",
    "    #normal_agent_path = '/kaggle/working'\n",
    "    tgz_agent_path = './'\n",
    "    normal_agent_path = './'\n",
    "    model_name = \"dqnv1\"\n",
    "    num_previous_observations = 0\n",
    "    epsilon = 0\n",
    "    init = False\n",
    "    debug = False\n",
    "\n",
    "    try:\n",
    "        model\n",
    "    except NameError:\n",
    "        init=True\n",
    "    else:\n",
    "        if model==None:\n",
    "            init = True \n",
    "            initializing\n",
    "    if init:\n",
    "        #initializations\n",
    "        defaults = [configuration.rows,\n",
    "                    configuration.columns,\n",
    "                    configuration.hunger_rate,\n",
    "                    configuration.min_food]\n",
    "\n",
    "        model = MyNN()\n",
    "        last_action = -1\n",
    "        last_observation = []\n",
    "        previous_observation = []\n",
    "        \n",
    "        file_name = os.path.join(normal_agent_path, f'{model_name}.pt')\n",
    "        if not os.path.exists(file_name):\n",
    "            file_name = os.path.join(tgz_agent_path, f'{model_name}.pt')\n",
    "            \n",
    "        model.load_state_dict(th.load(file_name), strict=False)\n",
    "        obs_prep = ObservationProcessor(configuration.rows, configuration.columns, configuration.hunger_rate, \n",
    "                                        configuration.min_food)\n",
    "    \n",
    "    #maintaint list of  last observations\n",
    "    if num_previous_observations>0 and len(last_observation)>0:\n",
    "        #Not initial step, t=0\n",
    "        previous_observation.append(last_observation)\n",
    "        #Keep list constrained to max length\n",
    "        if len(previous_observation)>num_previous_observations:\n",
    "            del previous_observation[0]\n",
    "            \n",
    "    #Convert to grid encoded with CellState values\n",
    "    aux_observation = [obs_prep.process_env_obs(observation)] \n",
    "    last_observation = aux_observation\n",
    "\n",
    "    if num_previous_observations>0 and len(previous_observation)==0:\n",
    "        #Initial step, t=0\n",
    "        previous_observation = [last_observation for _ in range(num_previous_observations)]\n",
    "\n",
    "    if num_previous_observations>0:\n",
    "        aux_observation = np.concatenate((*previous_observation, last_observation), axis=0)\n",
    "    else:\n",
    "        aux_observation = last_observation\n",
    "        \n",
    "    #predict with aux_observation.shape = (last_observations x rows x cols)\n",
    "    tensor_obs = th.Tensor([aux_observation])\n",
    "    n_out = model(tensor_obs) #Example: tensor([[0.2742, 0.2653, 0.2301, 0.2303]], grad_fn=<SoftmaxBackward>) \n",
    "    \n",
    "    #choose probabilistic next move based on prediction outputs\n",
    "    #with epsilon probability of fully random, always avoid opposite of last move\n",
    "    actions = [action.value for action in Action]\n",
    "    weights = list(n_out[0].detach().numpy())\n",
    "    if last_action!=-1:\n",
    "        #Avoid dying by stupidity xD\n",
    "        remove_index = actions.index(obs_prep.opposite(Action(last_action)).value)\n",
    "        del actions[remove_index]\n",
    "        del weights[remove_index]    \n",
    "    random=False\n",
    "\n",
    "    min_value = abs(min(weights))\n",
    "    weights = [min_value+w+1e-5 for w in weights] #Total of weights must be greater than zero  \n",
    "\n",
    "    \n",
    "    #Reduce weight to penalize bad moves (collisions, etc...)\n",
    "    weights_changed = False\n",
    "    weights_before = [w for w in weights]\n",
    "    for index, action in enumerate(actions):\n",
    "        future_position = obs_prep._translate(obs_prep.my_head, Action(action))\n",
    "        if not obs_prep.free_position(future_position):\n",
    "            weights[index] = min(weights[index], 1e-8) #Collision is worst case\n",
    "            weights_changed = True\n",
    "        elif future_position in obs_prep.dead_ends:\n",
    "            weights[index] = min(weights[index],1e-2) #dead ends\n",
    "            weights_changed = True\n",
    "        elif future_position in obs_prep.adjacent_to_heads:\n",
    "            weights[index] = min(weights[index],1e-8) #adjacent to heads\n",
    "            weights_changed = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    if debug and weights_changed:\n",
    "        print(aux_observation)\n",
    "        print(f'Adapted weights: before {weights_before} and after {weights} for actions {[Action(a).name for a in actions]}')\n",
    "    #elif debug and not weights_changed:\n",
    "    #    print(f'Action weights {weights}')\n",
    "\n",
    "    if rand.random() < epsilon:\n",
    "        prediction = rand.choice(actions)\n",
    "        random=True\n",
    "    else:\n",
    "        prediction = rand.choices(actions, weights=weights)[0] \n",
    "    action_predicted = Action(prediction).name\n",
    "    \n",
    "    #print(observation) #Uncomment to debug a bit too much...\n",
    "    #if (last_action!=-1) and debug:\n",
    "    #    print(last_observation)\n",
    "    #    print(f'valid_actions={actions}, w={weights}, chose={Action(prediction).name}, rand={random}',\n",
    "    #          f'previous={Action(last_action).name}, opposite={Action(obs_prep.opposite(Action(last_action)).value).name}') \n",
    "    \n",
    "    last_action = prediction\n",
    "    return action_predicted #return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"hungry_geese\", debug=True) #set debug to True to see agent internals each step\n",
    "\n",
    "env.reset()\n",
    "env.run([\"agent_dqn_1_1.py\", \"agent_dqn_1_1.py\", \"agent_dqn_1_1.py\", \"agent_dqn_1_1.py\"])\n",
    "env.render(mode=\"ipython\", width=700, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looool, our model is pretty shitty :)\n",
    "\n",
    "I did use a much smaller neural network than the original though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
